{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p0DzC9vVbnRkIqxom9dAfX1PqNS_OP8V",
      "authorship_tag": "ABX9TyPsOgzY9u0tyPIqjQ1zLe6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32fba5421be24bc1b39341d5215b74c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88a18f6a91c443e0bb9918e05f81064f",
              "IPY_MODEL_46a7155a959747f3bee90f202cf2fad6",
              "IPY_MODEL_b99680e719a94aefb29e5b1e382e2a18"
            ],
            "layout": "IPY_MODEL_543c41b197ce49f19504ea8453e80cea"
          }
        },
        "88a18f6a91c443e0bb9918e05f81064f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d29241a9b1f94a0cae9af9fb8194fceb",
            "placeholder": "​",
            "style": "IPY_MODEL_ec1cb4744c054ea7b91fbd7331cdbfde",
            "value": "Loading weights: 100%"
          }
        },
        "46a7155a959747f3bee90f202cf2fad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2863191211e14a5790e9211d05013661",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b982c813f4144348a56822c89fed8715",
            "value": 103
          }
        },
        "b99680e719a94aefb29e5b1e382e2a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16d643bce8e4404c904c974bd515f0e1",
            "placeholder": "​",
            "style": "IPY_MODEL_cf7c51bbd8a24773b622e4802b04505d",
            "value": " 103/103 [00:00&lt;00:00, 147.91it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "543c41b197ce49f19504ea8453e80cea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d29241a9b1f94a0cae9af9fb8194fceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1cb4744c054ea7b91fbd7331cdbfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2863191211e14a5790e9211d05013661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b982c813f4144348a56822c89fed8715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16d643bce8e4404c904c974bd515f0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7c51bbd8a24773b622e4802b04505d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajan-sharma-in/GDG/blob/main/AI_Buddy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hU-lI9_mRww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62174de-fea4-4dcf-ec81-5f9af8995358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.18.1 requires accelerate>=0.21.0, which is not installed.\n",
            "transformers 4.45.2 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 1.4.1 which is incompatible.\n",
            "tokenizers 0.20.1 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.4.1 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.18.1 requires accelerate>=0.21.0, which is not installed.\n",
            "gradio 6.6.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.26.2 which is incompatible.\n",
            "diffusers 0.36.0 requires huggingface-hub<2.0,>=0.34.0, but you have huggingface-hub 0.26.2 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Demo installs:\n",
        "# - Pin transformers to avoid: ImportError gather_state_dict_for_save\n",
        "# - Pin pillow to avoid random PIL typing issues\n",
        "\n",
        "%pip -q install -U faiss-cpu gradio pymupdf google-genai\n",
        "\n",
        "%pip -q install -U \\\n",
        "  \"transformers==4.45.2\" \\\n",
        "  \"sentence-transformers==3.1.1\" \\\n",
        "  \"tokenizers==0.20.1\" \\\n",
        "  \"safetensors==0.4.5\" \\\n",
        "  \"huggingface-hub==0.26.2\"\n",
        "\n",
        "%pip -q install -U \"pillow==10.4.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os, re, json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fitz  # PyMuPDF\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "QFVPpYPGn308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "d4ebbe43-43e8-4dfd-fad9-c27f761829e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'gather_state_dict_for_save' from 'transformers.integrations.tensor_parallel' (/usr/local/lib/python3.12/dist-packages/transformers/integrations/tensor_parallel.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-611692216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m from sentence_transformers.cross_encoder import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2096\u001b[0m                                                 \u001b[0mbase_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m                                             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m                                                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module_path} does not have {candidate_name} defined.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                                         \u001b[0;31m# Fallback: try via _class_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstring_to_operator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mversion_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2288\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2284\u001b[0m             \u001b[0;34m\"<=\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVersionComparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLESS_THAN_OR_EQUAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m         }\n\u001b[0;32m-> 2286\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstring_to_operator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mversion_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdpa_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdpa_attention_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdpa_paged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdpa_attention_paged_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m from .integrations.tensor_parallel import (\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mALL_PARALLEL_STYLES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0m_get_parameter_tp_plan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'gather_state_dict_for_save' from 'transformers.integrations.tensor_parallel' (/usr/local/lib/python3.12/dist-packages/transformers/integrations/tensor_parallel.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_DRIVE = True\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    PDF_DIR = Path(\"/content/drive/MyDrive/Colab_PDFs\")\n",
        "    CACHE_DIR = Path(\"/content/drive/MyDrive/Colab_PDFs/index_cache\")\n",
        "else:\n",
        "    PDF_DIR = Path(\"/content/pdfs\")\n",
        "    CACHE_DIR = Path(\"/content/index_cache\")\n",
        "\n",
        "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def list_pdfs():\n",
        "    pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
        "    print(f\"PDF folder: {PDF_DIR}\")\n",
        "    print(f\"Found {len(pdfs)} PDF(s).\")\n",
        "    for p in pdfs[:30]:\n",
        "        print(\" -\", p.name)\n",
        "    if len(pdfs) > 30:\n",
        "        print(\" ... (showing first 30)\")\n",
        "    return pdfs\n",
        "\n",
        "pdf_paths = list_pdfs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "KXEPKDzEn69p",
        "outputId": "57c62132-32e9-4d35-8d6a-26c69b177d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_Ink' from 'PIL._typing' (/usr/local/lib/python3.12/dist-packages/PIL/_typing.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2439213076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m from sentence_transformers.cross_encoder import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mverify_tp_plan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_flash_attention_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_import_flash_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_import_paged_flash_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_rope_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mROPE_INIT_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_d_fine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFineForObjectDetectionLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_d_fine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox_iou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_rt_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTDetrHungarianMatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRTDetrLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mImageInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         pil_torch_interpolation_mapping = {\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optical_flow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlyingChairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlyingThings3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHD1K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKittiFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSintel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from ._stereo_matching import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mCarlaStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mCREStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mETH3DStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/_optical_flow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecode_png\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_pfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mVideoMetaData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m from .image import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdecode_avif\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdecode_gif\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_load_library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPILLOW_VERSION_STRING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageText.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Ink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_Ink' from 'PIL._typing' (/usr/local/lib/python3.12/dist-packages/PIL/_typing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you use Google Drive for storing many big books:\n",
        "USE_DRIVE = True\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    PDF_DIR = Path(\"/content/drive/MyDrive/Colab_PDFs\")\n",
        "    CACHE_DIR = Path(\"/content/drive/MyDrive/Colab_PDFs/index_cache_v2\")\n",
        "else:\n",
        "    PDF_DIR = Path(\"/content/pdfs\")\n",
        "    CACHE_DIR = Path(\"/content/index_cache_v2\")\n",
        "\n",
        "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def list_pdfs():\n",
        "    pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
        "    print(\"PDF_DIR:\", PDF_DIR)\n",
        "    print(\"Found PDFs:\", len(pdfs))\n",
        "    for p in pdfs[:30]:\n",
        "        print(\" -\", p.name)\n",
        "    if len(pdfs) > 30:\n",
        "        print(\" ... (showing first 30)\")\n",
        "    return pdfs\n",
        "\n",
        "pdf_paths = list_pdfs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdxByrIn94_",
        "outputId": "6822441e-4bef-46dc-95de-e2e122847732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "PDF_DIR: /content/drive/MyDrive/Colab_PDFs\n",
            "Found PDFs: 1\n",
            " - 16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — Preprocess + Chunking\n",
        "# - Cleans whitespace\n",
        "# - Splits into overlapping character chunks\n",
        "# - Extracts chapter-like headings for rule-based queries\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 180\n",
        "MAX_CHUNKS = 15000  # guardrail for very large corpora\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "def make_chunks(df_pages, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    chunks = []\n",
        "    for _, row in df_pages.iterrows():\n",
        "        text = clean_text(row[\"text\"])\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            chunk_text = text[start:end]\n",
        "            chunks.append({\n",
        "                \"chunk_id\": f\"{row.file_name}__p{row.page_number}__{start}\",\n",
        "                \"file_name\": row.file_name,\n",
        "                \"page_number\": row.page_number,\n",
        "                \"chunk_text\": chunk_text,\n",
        "                \"low_text\": row.low_text,\n",
        "            })\n",
        "            start = end - overlap\n",
        "    return pd.DataFrame(chunks)\n",
        "\n",
        "def extract_chapter_candidates(df_pages):\n",
        "    heads = []\n",
        "    pattern = re.compile(r\"\\bchap(?:ter)?\\b\\s*[0-9ivxlcdm]*\", re.IGNORECASE)\n",
        "    for _, row in df_pages.iterrows():\n",
        "        for line in row[\"text\"].splitlines():\n",
        "            clean = line.strip()\n",
        "            if not clean or len(clean) > 90:\n",
        "                continue\n",
        "            if pattern.search(clean) or (\"chapter\" in clean.lower() and len(clean.split()) <= 8):\n",
        "                heads.append({\n",
        "                    \"file_name\": row.file_name,\n",
        "                    \"page_number\": row.page_number,\n",
        "                    \"heading\": clean_text(clean),\n",
        "                })\n",
        "    df = pd.DataFrame(heads)\n",
        "    if len(df):\n",
        "        df = df.drop_duplicates(subset=[\"file_name\", \"heading\"], keep=\"first\").sort_values([\"file_name\", \"page_number\"])\n",
        "    return df\n",
        "\n",
        "if len(pages_df) == 0:\n",
        "    chunks_df = pd.DataFrame(columns=[\"chunk_id\",\"file_name\",\"page_number\",\"chunk_text\",\"low_text\"])\n",
        "    chapter_df = pd.DataFrame(columns=[\"file_name\",\"page_number\",\"heading\"])\n",
        "else:\n",
        "    chunks_df = make_chunks(pages_df)\n",
        "    chapter_df = extract_chapter_candidates(pages_df)\n",
        "    if len(chunks_df) > MAX_CHUNKS:\n",
        "        print(f\"Too many chunks ({len(chunks_df)}). Consider fewer PDFs or larger chunk_size.\")\n",
        "    print(f\"Chunks created: {len(chunks_df)} | Avg chars/chunk: {chunks_df['chunk_text'].str.len().mean():.1f}\")\n",
        "    print(f\"Chapter-like headings detected: {len(chapter_df)}\")\n",
        "\n",
        "chunks_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ffjI9LkOoP26",
        "outputId": "2c38be35-4003-4c50-e03a-f2887ff13dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks created: 3502 | Avg chars/chunk: 846.2\n",
            "Chapter-like headings detected: 494\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            chunk_id  \\\n",
              "0  16_EBOOK-7th_ed_software_engineering_a_practit...   \n",
              "1  16_EBOOK-7th_ed_software_engineering_a_practit...   \n",
              "2  16_EBOOK-7th_ed_software_engineering_a_practit...   \n",
              "3  16_EBOOK-7th_ed_software_engineering_a_practit...   \n",
              "4  16_EBOOK-7th_ed_software_engineering_a_practit...   \n",
              "\n",
              "                                           file_name  page_number  \\\n",
              "0  16_EBOOK-7th_ed_software_engineering_a_practit...            1   \n",
              "1  16_EBOOK-7th_ed_software_engineering_a_practit...            2   \n",
              "2  16_EBOOK-7th_ed_software_engineering_a_practit...            4   \n",
              "3  16_EBOOK-7th_ed_software_engineering_a_practit...            5   \n",
              "4  16_EBOOK-7th_ed_software_engineering_a_practit...            5   \n",
              "\n",
              "                                          chunk_text  low_text  \n",
              "0  Software Engineering A Practitioner’s Approach...     False  \n",
              "1  Software Engineering A P R A C T I T I O N E R...     False  \n",
              "2  Software Engineering A P R A C T I T I O N E R...     False  \n",
              "3  SOFTWARE ENGINEERING: A PRACTITIONER’S APPROAC...     False  \n",
              "4  0 DOC/DOC 0 9 ISBN 978–0–07–337597–7 MHID 0–07...     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-275d6868-52f5-4fbc-9ae0-0cba6c93a2d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>file_name</th>\n",
              "      <th>page_number</th>\n",
              "      <th>chunk_text</th>\n",
              "      <th>low_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>1</td>\n",
              "      <td>Software Engineering A Practitioner’s Approach...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>2</td>\n",
              "      <td>Software Engineering A P R A C T I T I O N E R...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>4</td>\n",
              "      <td>Software Engineering A P R A C T I T I O N E R...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>5</td>\n",
              "      <td>SOFTWARE ENGINEERING: A PRACTITIONER’S APPROAC...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>16_EBOOK-7th_ed_software_engineering_a_practit...</td>\n",
              "      <td>5</td>\n",
              "      <td>0 DOC/DOC 0 9 ISBN 978–0–07–337597–7 MHID 0–07...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-275d6868-52f5-4fbc-9ae0-0cba6c93a2d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-275d6868-52f5-4fbc-9ae0-0cba6c93a2d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-275d6868-52f5-4fbc-9ae0-0cba6c93a2d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "chunks_df",
              "summary": "{\n  \"name\": \"chunks_df\",\n  \"rows\": 3502,\n  \"fields\": [\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3502,\n        \"samples\": [\n          \"16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf__p836__2460\",\n          \"16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf__p94__0\",\n          \"16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf__p578__3280\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 265,\n        \"min\": 1,\n        \"max\": 930,\n        \"num_unique_values\": 919,\n        \"samples\": [\n          75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3502,\n        \"samples\": [\n          \"ces on software process improvement is available on the Internet. An up-to-date list of World Wide Web references relevant to SPI can be found at the SEPA website: www.mhhe.com/engcs/compsci/pressman/professional/olc/ser.htm. CHAPTER 30 SOFTWARE PROCESS IMPROVEMENT 807\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"low_text\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — Embeddings + FAISS Index\n",
        "# - Loads MiniLM model\n",
        "# - Builds/loads FAISS cosine index\n",
        "# - Caches index + metadata to /content/index_cache\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "INDEX_PATH = CACHE_DIR / \"faiss.index\"\n",
        "META_PATH = CACHE_DIR / \"chunks.parquet\"\n",
        "EMB_PATH = CACHE_DIR / \"embeddings.npy\"\n",
        "MANIFEST_PATH = CACHE_DIR / \"manifest.json\"\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "def current_manifest(pdf_paths):\n",
        "    return {\n",
        "        str(p.name): {\"mtime\": p.stat().st_mtime, \"size\": p.stat().st_size}\n",
        "        for p in pdf_paths\n",
        "    }\n",
        "\n",
        "def load_cache(pdf_paths):\n",
        "    if not (INDEX_PATH.exists() and META_PATH.exists() and EMB_PATH.exists() and MANIFEST_PATH.exists()):\n",
        "        return None\n",
        "    saved = json.load(open(MANIFEST_PATH))\n",
        "    if saved != current_manifest(pdf_paths):\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_parquet(META_PATH)\n",
        "        emb = np.load(EMB_PATH)\n",
        "        idx = faiss.read_index(str(INDEX_PATH))\n",
        "        print(\"Loaded cached index.\")\n",
        "        return df, emb, idx\n",
        "    except Exception as e:\n",
        "        print(\"Cache load failed:\", e)\n",
        "        return None\n",
        "\n",
        "def save_cache(df, emb, idx, pdf_paths):\n",
        "    df.to_parquet(META_PATH, index=False)\n",
        "    np.save(EMB_PATH, emb)\n",
        "    faiss.write_index(idx, str(INDEX_PATH))\n",
        "    json.dump(current_manifest(pdf_paths), open(MANIFEST_PATH, \"w\"))\n",
        "    print(\"Cache saved to /content/index_cache.\")\n",
        "\n",
        "def build_index(pdf_paths, force=False):\n",
        "    if len(pdf_paths) == 0:\n",
        "        print(\"No PDFs to index.\")\n",
        "        return None, None, None\n",
        "    if not force:\n",
        "        cached = load_cache(pdf_paths)\n",
        "        if cached:\n",
        "            return cached\n",
        "\n",
        "    print(\"Encoding chunks...\")\n",
        "    if len(chunks_df) == 0:\n",
        "        print(\"No chunks to index yet.\")\n",
        "        return None, None, None\n",
        "\n",
        "    emb = model.encode(\n",
        "        chunks_df[\"chunk_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    ).astype(\"float32\")\n",
        "    faiss.normalize_L2(emb)\n",
        "\n",
        "    dim = emb.shape[1]\n",
        "    idx = faiss.IndexFlatIP(dim)\n",
        "    idx.add(emb)\n",
        "    save_cache(chunks_df, emb, idx, pdf_paths)\n",
        "    return chunks_df, emb, idx\n",
        "\n",
        "chunks_df, emb_matrix, index = build_index(pdf_paths, force=False)\n",
        "if index:\n",
        "    print(f\"FAISS ready | dim: {emb_matrix.shape[1]} | vectors: {index.ntotal}\")\n",
        "    print(f\"Indexed PDFs: {len(pdf_paths)}, Total pages: {len(pages_df)}, Total chunks: {len(chunks_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "32fba5421be24bc1b39341d5215b74c6",
            "88a18f6a91c443e0bb9918e05f81064f",
            "46a7155a959747f3bee90f202cf2fad6",
            "b99680e719a94aefb29e5b1e382e2a18",
            "543c41b197ce49f19504ea8453e80cea",
            "d29241a9b1f94a0cae9af9fb8194fceb",
            "ec1cb4744c054ea7b91fbd7331cdbfde",
            "2863191211e14a5790e9211d05013661",
            "b982c813f4144348a56822c89fed8715",
            "16d643bce8e4404c904c974bd515f0e1",
            "cf7c51bbd8a24773b622e4802b04505d"
          ]
        },
        "id": "G4bftr4OobDO",
        "outputId": "e807ad09-9c7f-4ec7-b150-497174bc7b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32fba5421be24bc1b39341d5215b74c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached index.\n",
            "FAISS ready | dim: 384 | vectors: 3502\n",
            "Indexed PDFs: 1, Total pages: 930, Total chunks: 3502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — Static Retrieval Demo\n",
        "# - Retrieves top-K chunks\n",
        "# - Detects chapter/page-count/list-doc/author queries for routing\n",
        "\n",
        "def confidence_label(score: float) -> str:\n",
        "    if score >= 0.65: return \"High\"\n",
        "    if score >= 0.40: return \"Medium\"\n",
        "    return \"Low\"\n",
        "\n",
        "def preview(text, n=240):\n",
        "    return (text[:n] + \" ...\") if len(text) > n else text\n",
        "\n",
        "def is_chapter_query(q: str) -> bool:\n",
        "    return bool(re.search(r\"chapter( names?| list| titles?)|list .*chapters|all chapters\", q, re.IGNORECASE))\n",
        "\n",
        "def is_pagecount_query(q: str) -> bool:\n",
        "    return bool(re.search(r\"how many pages|page count|total pages|number of pages\", q, re.IGNORECASE))\n",
        "\n",
        "def is_list_docs_query(q: str) -> bool:\n",
        "    return bool(re.search(r\"list (pdfs|books|documents)|what (files|pdfs)|which books\", q, re.IGNORECASE))\n",
        "\n",
        "def is_author_query(q: str) -> bool:\n",
        "    return bool(re.search(r\"who is the author|author of|who wrote|written by\", q, re.IGNORECASE))\n",
        "\n",
        "def retrieve_chunks(query: str, k: int = 3, doc_filter: str = \"All\"):\n",
        "    if index is None:\n",
        "        return {\"error\": \"Index not built yet.\"}\n",
        "    base_df = chunks_df if doc_filter == \"All\" else chunks_df[chunks_df.file_name == doc_filter]\n",
        "    if len(base_df) == 0:\n",
        "        return {\"error\": f\"No chunks for '{doc_filter}'. Rebuild index or pick All.\"}\n",
        "\n",
        "    q = clean_text(query)\n",
        "    q_emb = model.encode([q], convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n",
        "    faiss.normalize_L2(q_emb)\n",
        "\n",
        "    if doc_filter == \"All\":\n",
        "        k_eff = min(k, len(base_df))\n",
        "        scores, idxs = index.search(q_emb, k_eff)\n",
        "        rows = base_df.iloc[idxs[0]]\n",
        "    else:\n",
        "        sub = base_df.reset_index(drop=True)\n",
        "        sub_emb = emb_matrix[sub.index]\n",
        "        sub_index = faiss.IndexFlatIP(sub_emb.shape[1])\n",
        "        sub_index.add(sub_emb)\n",
        "        k_eff = min(k, len(sub))\n",
        "        scores, idxs = sub_index.search(q_emb, k_eff)\n",
        "        rows = sub.iloc[idxs[0]]\n",
        "\n",
        "    items = []\n",
        "    for r, row in enumerate(rows.itertuples()):\n",
        "        score = float(scores[0][r])\n",
        "        items.append({\n",
        "            \"score\": score,\n",
        "            \"confidence\": confidence_label(score),\n",
        "            \"file_name\": row.file_name,\n",
        "            \"page_number\": int(row.page_number),\n",
        "            \"chunk_text\": row.chunk_text,\n",
        "            \"preview\": preview(row.chunk_text),\n",
        "        })\n",
        "    return {\"error\": None, \"items\": items, \"top_score\": float(scores[0][0]) if len(items) else 0.0}\n",
        "\n",
        "# quick smoke test\n",
        "demo = retrieve_chunks(\"Quality Concept\", k=3)\n",
        "demo[\"items\"][:1] if demo[\"error\"] is None else demo\n"
      ],
      "metadata": {
        "id": "EVPwvSOXoeg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a96814-9b5f-4726-9605-1afa0562283a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.7885301113128662,\n",
              "  'confidence': 'High',\n",
              "  'file_name': '16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf',\n",
              "  'page_number': 440,\n",
              "  'chunk_text': ' CHAPTER 14 QUALITY CONCEPTS 411',\n",
              "  'preview': ' CHAPTER 14 QUALITY CONCEPTS 411'}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — Pipeline functions (retrieve + rag + metadata routes)\n",
        "# - Rebuilds index\n",
        "# - Routes metadata queries (page count, list docs, chapters, author) before embeddings\n",
        "\n",
        "def rebuild_index(force=False):\n",
        "    global pdf_paths, pages_df, chunks_df, emb_matrix, index, chapter_df, doc_stats\n",
        "    pdf_paths = list_pdfs()\n",
        "    if len(pdf_paths) == 0:\n",
        "        return \"No PDFs found. Upload files first.\"\n",
        "    pages_df, doc_stats = extract_pdfs(pdf_paths)\n",
        "    chunks_df = make_chunks(pages_df)\n",
        "    chapter_df = extract_chapter_candidates(pages_df)\n",
        "    if len(chunks_df) > MAX_CHUNKS:\n",
        "        return f\"Too many chunks ({len(chunks_df)}). Use fewer PDFs.\"\n",
        "    chunks_df, emb_matrix, index = build_index(pdf_paths, force=force)\n",
        "    if index:\n",
        "        return f\"Rebuilt index. PDFs: {len(pdf_paths)}, pages: {len(pages_df)}, chunks: {len(chunks_df)}, chapters: {len(chapter_df)}\"\n",
        "    return \"Index rebuild failed.\"\n",
        "\n",
        "def ensure_index_ready():\n",
        "    if 'index' not in globals() or index is None or 'pages_df' not in globals() or pages_df is None:\n",
        "        return rebuild_index(force=True)\n",
        "    return \"Index ready (cache loaded).\"\n",
        "\n",
        "def sync_pdfs():\n",
        "    # Smart sync: reuse cache when unchanged; rebuild when new/modified files\n",
        "    return rebuild_index(force=False)\n",
        "\n",
        "def chapter_answer(doc_filter=\"All\"):\n",
        "    if chapter_df is None or len(chapter_df) == 0:\n",
        "        return \"No chapter-like headings found.\", \"\", \"\"\n",
        "    df = chapter_df if doc_filter == \"All\" else chapter_df[chapter_df.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No chapter headings in {doc_filter}.\", \"\", \"\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        cite = f\"[{row.file_name} p.{row.page_number}]\"\n",
        "        lines.append(f\"{cite} {row.heading}\")\n",
        "    answer_text = \"Chapter-like headings I found:\\n\" + \"\\n\".join(lines)\n",
        "    transparency = \"\\n\".join(lines)\n",
        "    prompt = \"Rule-based chapter heading extraction (no LLM).\"\n",
        "    return answer_text, \"Metadata\", transparency\n",
        "\n",
        "def pagecount_answer(doc_filter=\"All\"):\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\"\n",
        "    df = doc_stats if doc_filter == \"All\" else doc_stats[doc_stats.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No stats for {doc_filter}.\", \"\", \"\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        lines.append(f\"{row.file_name}: {row.total_pages} pages (low-text {row.low_text_pages})\")\n",
        "    ans = \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans\n",
        "\n",
        "def listdocs_answer():\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\"\n",
        "    lines = [f\"{row.file_name} ({row.total_pages} pages)\" for row in doc_stats.itertuples()]\n",
        "    ans = \"Available PDFs:\\n\" + \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans\n",
        "\n",
        "def author_answer(doc_filter=\"All\"):\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\"\n",
        "    df = doc_stats if doc_filter == \"All\" else doc_stats[doc_stats.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No author info for {doc_filter}.\", \"\", \"\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        lines.append(f\"{row.file_name}: {row.author} (Title: {row.title})\")\n",
        "    ans = \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans\n",
        "\n",
        "def rag_answer(question, items, use_gemini=False):\n",
        "    context = \"\\n\\n\".join([item[\"chunk_text\"] for item in items])\n",
        "    if not use_gemini:\n",
        "        answer = \"I found the following information in the documents:\\n\\n\" + context\n",
        "        prompt = \"Direct embedding retrieval (no LLM).\"\n",
        "        return answer, prompt\n",
        "    else:\n",
        "        # Attempt to use Gemini model if it's available\n",
        "        if 'genai' in globals() and 'gemini_model' in globals():\n",
        "            try:\n",
        "                gemini_prompt = f\"\"\"\n",
        "                Based ONLY on the following information, provide a helpful and conversational response to the query.\n",
        "                Do NOT include any information not present in the provided context.\n",
        "\n",
        "                Query: \"{question}\"\n",
        "\n",
        "                Information from documents:\n",
        "                {context}\n",
        "\n",
        "                Please synthesize this information and provide a clear answer.\n",
        "                \"\"\"\n",
        "                response = gemini_model.generate_content(gemini_prompt)\n",
        "                answer = response.text\n",
        "                prompt = \"Gemini RAG with context.\"\n",
        "                return answer, prompt\n",
        "            except Exception as e:\n",
        "                answer = f\"Error with Gemini: {e}. Returning raw chunks: \\n\\n\" + context\n",
        "                prompt = \"Gemini error, falling back to direct chunks.\"\n",
        "                return answer, prompt\n",
        "        else:\n",
        "            answer = \"Gemini not configured or initialized. Returning raw chunks: \\n\\n\" + context\n",
        "            prompt = \"Gemini not available, falling back to direct chunks.\"\n",
        "            return answer, prompt\n",
        "\n",
        "def pipeline(question, top_k=3, doc_filter=\"All\", use_gemini=False):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\", \"\", \"\"\n",
        "\n",
        "    # ensure index/data available (lazy init)\n",
        "    ready_msg = ensure_index_ready()\n",
        "\n",
        "    if is_list_docs_query(question):\n",
        "        ans, conf, trans = listdocs_answer()\n",
        "        return ans, conf, trans, \"List docs route\"\n",
        "    if is_pagecount_query(question):\n",
        "        ans, conf, trans = pagecount_answer(doc_filter)\n",
        "        return ans, conf, trans, \"Page-count route\"\n",
        "    if is_author_query(question):\n",
        "        ans, conf, trans = author_answer(doc_filter)\n",
        "        return ans, conf, trans, \"Author route\"\n",
        "    if is_chapter_query(question):\n",
        "        ans, conf, trans = chapter_answer(doc_filter)\n",
        "        return ans, conf, trans, \"Chapter route\"\n",
        "\n",
        "    res = retrieve_chunks(question, k=int(top_k), doc_filter=doc_filter)\n",
        "    if res[\"error\"]:\n",
        "        return res[\"error\"], \"\", \"\", \"\"\n",
        "    items = res[\"items\"]\n",
        "    top = items[0]\n",
        "    conf = f\"{confidence_label(top['score'])} (top score {top['score']:.3f})\"\n",
        "    answer_text, prompt = rag_answer(question, items, use_gemini=use_gemini)\n",
        "    lines = []\n",
        "    for it in items:\n",
        "        cite = f\"[{it['file_name']} p.{it['page_number']}]\"\n",
        "        lines.append(f\"Score: {it['score']:.3f} | {it['confidence']} | {cite}\\n{it['preview']}\")\n",
        "    transparency = \"\\n\\n\".join(lines)\n",
        "    return answer_text, conf, transparency, prompt\n",
        "\n",
        "print(\"Call pipeline(question, top_k, doc_filter, use_gemini) or sync_pdfs() / rebuild_index(force=True).\")"
      ],
      "metadata": {
        "id": "wH_9JspNo2Gz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e64b85-e38b-442e-c750-d7f1304e1f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call pipeline(question, top_k, doc_filter, use_gemini) or sync_pdfs() / rebuild_index(force=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69501719",
        "outputId": "966ae63b-31b9-4b1b-d3f9-ece2d9351773"
      },
      "source": [
        "# Cell X — Embedding Search Result Demo\n",
        "# This cell demonstrates retrieving embedding search results for a specific question.\n",
        "\n",
        "print(\"Running embedding search for the question...\")\n",
        "question = \"what is software testing strategies\"\n",
        "\n",
        "# Check if pipeline is defined, otherwise provide instructions\n",
        "if 'pipeline' not in globals():\n",
        "    print(\"Error: The 'pipeline' function is not defined. Please ensure Cell 8 ('wH_9JspNo2Gz') has been executed.\")\n",
        "    answer, confidence, transparency, grounding = \"Error: pipeline not defined. Run Cell 8.\", \"\", \"\", \"\"\n",
        "else:\n",
        "    # Call pipeline with use_gemini=False to see direct embedding retrieval results\n",
        "    answer, confidence, transparency, grounding = pipeline(question, top_k=3, doc_filter=\"All\", use_gemini=False)\n",
        "\n",
        "print(\"\\n--- Question ---\")\n",
        "print(question)\n",
        "print(\"\\n--- Answer (from Embedding Retrieval) ---\")\n",
        "print(answer)\n",
        "print(\"\\n--- Confidence ---\")\n",
        "print(confidence)\n",
        "print(\"\\n--- Retrieval Transparency (Embeddings Results) ---\")\n",
        "print(transparency)\n",
        "print(\"\\n--- Grounding Prompt / Route ---\")\n",
        "print(grounding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running embedding search for the question...\n",
            "\n",
            "--- Question ---\n",
            "what is software testing strategies\n",
            "\n",
            "--- Answer (from Embedding Retrieval) ---\n",
            "I found the following information in the documents:\n",
            "\n",
            "A strategy for software testing provides a road map that describes the steps to be conducted as part of testing, when these steps are planned and then undertaken, and how much effort, time, and resources will be required. Therefore, any testing strategy must incorporate test planning, test case design, test execution, and resultant data collection and evaluation. A software testing strategy should be flexible enough to promote a customized testing approach. At the same time, it must be rigid enough to encourage reason- able planning and management tracking as the project progresses. Shooman [Sho83] discusses these issues: In many ways, testing is an individualistic process, and the number of different types of tests varies as much as the different development approaches. For many years, our 449 C H A P T E R 17 SOFTWARE TESTING STRATEGIES What is it? Software is tested to uncover errors that were made inad- vertently as it was designed and constructed. But how do you conduct the tests?\n",
            "\n",
            "17 SOFTWARE TESTING STRATEGIES What is it? Software is tested to uncover errors that were made inad- vertently as it was designed and constructed. But how do you conduct the tests? Should you develop a formal plan for your tests? Should you test the entire program as a whole or run tests only on a small part of it? Should you rerun tests you’ve already conducted as you add new components to a large system? When should you involve the customer? These and many other questions are answered when you develop a software testing strategy. Who does it? A strategy for software testing is developed by the project manager, software engineers, and testing specialists. Why is it important? Testing often accounts for more project effort than any other software engi- neering action. If it is conducted haphazardly, time is wasted, unnecessary effort is expended, and even worse, errors sneak through undetected. It would therefore seem reasonable to establish a systematic strategy for testing software. \n",
            "\n",
            "450 PART THREE QUALITY MANAGEMENT integration testing . . . . . . . . .459 regression testing . . . . . . . . .467 system testing . . .470 unit testing . . . . .456 validation testing . . . . . . . . .467 V&V . . . . . . . . . .450 only defense against programming errors was careful design and the native intelligence of the programmer. We are now in an era in which modern design techniques [and tech- nical reviews] are helping us to reduce the number of initial errors that are inherent in the code. Similarly, different test methods are beginning to cluster themselves into several distinct approaches and philosophies. These “approaches and philosophies” are what I call strategy—the topic to be pre- sented in this chapter. In Chapters 18 through 20, the testing methods and tech- niques that implement the strategy are presented. 17.1 A STRATEGIC APPROACH TO SOFTWARE TESTING Testing is a set of activities that can be planned in advance and conducted system- atically. For this reason a temp\n",
            "\n",
            "--- Confidence ---\n",
            "High (top score 0.812)\n",
            "\n",
            "--- Retrieval Transparency (Embeddings Results) ---\n",
            "Score: 0.812 | High | [16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf p.478]\n",
            "A strategy for software testing provides a road map that describes the steps to be conducted as part of testing, when these steps are planned and then undertaken, and how much effort, time, and resources will be required. Therefore, any tes ...\n",
            "\n",
            "Score: 0.789 | High | [16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf p.478]\n",
            "17 SOFTWARE TESTING STRATEGIES What is it? Software is tested to uncover errors that were made inad- vertently as it was designed and constructed. But how do you conduct the tests? Should you develop a formal plan for your tests? Should you ...\n",
            "\n",
            "Score: 0.741 | High | [16_EBOOK-7th_ed_software_engineering_a_practitioners_approach_by_roger_s._pressman_.pdf p.479]\n",
            "450 PART THREE QUALITY MANAGEMENT integration testing . . . . . . . . .459 regression testing . . . . . . . . .467 system testing . . .470 unit testing . . . . .456 validation testing . . . . . . . . .467 V&V . . . . . . . . . .450 only def ...\n",
            "\n",
            "--- Grounding Prompt / Route ---\n",
            "Direct embedding retrieval (no LLM).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new-cell-1",
        "outputId": "fc992a9e-4dd9-471e-a2d7-c06459f779a8"
      },
      "source": [
        "!pip -q install -U google-genai\n",
        "\n",
        "import random, time\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "assert API_KEY, \"Set GOOGLE_API_KEY in Colab Secrets\"\n",
        "\n",
        "client = genai.Client(\n",
        "    api_key=API_KEY,\n",
        "    http_options=types.HttpOptions(\n",
        "        api_version=\"v1\",\n",
        "        timeout=90_000,  # milliseconds\n",
        "        retry_options=types.HttpRetryOptions(\n",
        "            attempts=4,          # total attempts including first try\n",
        "            initial_delay=1.0,\n",
        "            max_delay=15.0,\n",
        "            # default retryable codes include 429 and 5xx in this SDK\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "current_question = globals().get(\"question\")\n",
        "assert current_question, \"Run the embedding cell first so 'question' exists.\"\n",
        "\n",
        "answer = globals().get(\"answer\", \"\")\n",
        "confidence = globals().get(\"confidence\", \"\")\n",
        "transparency = globals().get(\"transparency\", \"\")\n",
        "grounding = globals().get(\"grounding\", \"\")\n",
        "\n",
        "def clip(x, n=3500):\n",
        "    x = \"\" if x is None else str(x)\n",
        "    return x[:n] + (\"…[trimmed]\" if len(x) > n else \"\")\n",
        "\n",
        "context = \"\\n\\n\".join([\n",
        "    f\"Answer:\\n{clip(answer, 4000)}\",\n",
        "    f\"Confidence:\\n{clip(confidence, 500)}\",\n",
        "    f\"Transparency:\\n{clip(transparency, 4000)}\",\n",
        "    f\"Route:\\n{clip(grounding, 1200)}\",\n",
        "])\n",
        "\n",
        "prompt = (\n",
        "    \"Use ONLY the context below to answer the query. If context is insufficient, say so.\\n\\n\"\n",
        "    f\"Query: {current_question}\\n\\n\"\n",
        "    f\"Context:\\n{context}\"\n",
        ")\n",
        "\n",
        "# streaming output\n",
        "print(\"---- Gemini output ----\")\n",
        "out = []\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=1024,\n",
        "        temperature=0.2,\n",
        "    ),\n",
        "):\n",
        "    if chunk.text:\n",
        "        print(chunk.text, end=\"\", flush=True)\n",
        "        out.append(chunk.text)\n",
        "\n",
        "print()\n",
        "final_text = \"\".join(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Gemini output ----\n",
            "A software testing strategy provides a roadmap that outlines the steps to be conducted as part of testing, including when these steps are planned and undertaken, and the required effort, time, and resources. It must incorporate test planning, test case design, test execution, and the collection and evaluation of resultant data.\n",
            "\n",
            "A testing strategy should be flexible enough to allow for customized testing approaches, yet rigid enough to encourage reasonable planning and management tracking throughout the project. It addresses questions such as how to conduct tests, whether to develop a formal plan, if the entire program or only parts should be tested, when to rerun tests, and when to involve the customer. Essentially, it encompasses the approaches and philosophies for systematically planning and conducting testing activities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65d42d0e"
      },
      "source": [
        "# Task\n",
        "Refactor the RAG pipeline by splitting the `pipeline` function in Cell 8 into a `pipeline_retrieve_and_route` function that solely handles retrieval and routing (returning raw results without LLM synthesis), remove `rag_answer`, and then update the `ui_query` function in Cell 11 to orchestrate the full RAG workflow: call `pipeline_retrieve_and_route` and conditionally use Gemini for synthesis. Additionally, modify the Gradio UI in Cell 11 to include a new textbox for `raw_retrieved_context_display` to show the raw context alongside the synthesized answer, ensuring all new return values are correctly mapped and displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19adae86"
      },
      "source": [
        "## Modify Retrieval and Routing Function\n",
        "\n",
        "### Subtask:\n",
        "Modify Cell 8 (`wH_9JspNo2Gz`). The `pipeline` function will be renamed to `pipeline_retrieve_and_route` and will be strictly responsible for retrieving raw document chunks or metadata answers. It will *not* call the Gemini model for synthesis. It will return `(retrieval_output, confidence, transparency, raw_context_for_llm, retrieval_route)`. The `rag_answer` function will be removed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af3977bf"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying an existing code cell (`wH_9JspNo2Gz`) by renaming a function, removing another, and adjusting return values and a print statement as per the instructions. This single code block will apply all the requested changes to the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefac138",
        "outputId": "8aad9904-89c9-4f73-a978-b51fdc1998cf"
      },
      "source": [
        "# Cell 8 — Pipeline functions (retrieve + rag + metadata routes)\n",
        "# - Rebuilds index\n",
        "# - Routes metadata queries (page count, list docs, chapters, author) before embeddings\n",
        "# - If use_gemini=True: runs context-only RAG using google-genai (Fix 2)\n",
        "\n",
        "# If you haven't installed yet:\n",
        "# !pip -q install -U google-genai\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# ----------------------------\n",
        "# Gemini client (Fix 2) — cached\n",
        "# ----------------------------\n",
        "_GENAI_CLIENT = None\n",
        "\n",
        "def get_genai_client():\n",
        "    global _GENAI_CLIENT\n",
        "    if _GENAI_CLIENT is not None:\n",
        "        return _GENAI_CLIENT\n",
        "\n",
        "    API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    assert API_KEY, \"Set GOOGLE_API_KEY in Colab Secrets\"\n",
        "\n",
        "    _GENAI_CLIENT = genai.Client(\n",
        "        api_key=API_KEY,\n",
        "        http_options=types.HttpOptions(\n",
        "            api_version=\"v1\",\n",
        "            timeout=90_000,  # ms\n",
        "            retry_options=types.HttpRetryOptions(\n",
        "                attempts=4,\n",
        "                initial_delay=1.0,\n",
        "                max_delay=15.0,\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    return _GENAI_CLIENT\n",
        "\n",
        "def _clip(x, n=6000):\n",
        "    x = \"\" if x is None else str(x)\n",
        "    return x[:n] + (\"…[trimmed]\" if len(x) > n else \"\")\n",
        "\n",
        "def rag_with_gemini_from_context(\n",
        "    question: str,\n",
        "    raw_context: str,\n",
        "    transparency: str = \"\",\n",
        "    model: str = \"gemini-2.5-flash\",\n",
        "    max_output_tokens: int = 512,\n",
        "    temperature: float = 0.2,\n",
        "    stream: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Context-only response: Gemini must use ONLY provided context.\n",
        "    Uses timeout + retries configured on the client.\n",
        "    \"\"\"\n",
        "    client = get_genai_client()\n",
        "\n",
        "    # Keep context compact. Big PDF excerpts often cause slow/hangs.\n",
        "    ctx = _clip(raw_context, 12000)\n",
        "    trans = _clip(transparency, 4000)\n",
        "\n",
        "    prompt = (\n",
        "        \"Answer the query using ONLY the context below.\\n\"\n",
        "        \"If the context does not contain the answer, say: 'Not found in the provided context.'\\n\\n\"\n",
        "        f\"Query:\\n{question}\\n\\n\"\n",
        "        f\"Context:\\n{ctx}\\n\\n\"\n",
        "        f\"Retrieval Notes (optional):\\n{trans}\\n\"\n",
        "    )\n",
        "\n",
        "    cfg = types.GenerateContentConfig(\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    if stream:\n",
        "        out = []\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=prompt,\n",
        "            config=cfg,\n",
        "        ):\n",
        "            if chunk.text:\n",
        "                print(chunk.text, end=\"\", flush=True)\n",
        "                out.append(chunk.text)\n",
        "        print()\n",
        "        return \"\".join(out)\n",
        "    else:\n",
        "        resp = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=prompt,\n",
        "            config=cfg,\n",
        "        )\n",
        "        return getattr(resp, \"text\", \"\") or \"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Your existing index helpers\n",
        "# ----------------------------\n",
        "def rebuild_index(force=False):\n",
        "    global pdf_paths, pages_df, chunks_df, emb_matrix, index, chapter_df, doc_stats\n",
        "    pdf_paths = list_pdfs()\n",
        "    if len(pdf_paths) == 0:\n",
        "        return \"No PDFs found. Upload files first.\"\n",
        "    pages_df, doc_stats = extract_pdfs(pdf_paths)\n",
        "    chunks_df = make_chunks(pages_df)\n",
        "    chapter_df = extract_chapter_candidates(pages_df)\n",
        "    if len(chunks_df) > MAX_CHUNKS:\n",
        "        return f\"Too many chunks ({len(chunks_df)}). Use fewer PDFs.\"\n",
        "    chunks_df, emb_matrix, index = build_index(pdf_paths, force=force)\n",
        "    if index:\n",
        "        return f\"Rebuilt index. PDFs: {len(pdf_paths)}, pages: {len(pages_df)}, chunks: {len(chunks_df)}, chapters: {len(chapter_df)}\"\n",
        "    return \"Index rebuild failed.\"\n",
        "\n",
        "def ensure_index_ready():\n",
        "    if 'index' not in globals() or index is None or 'pages_df' not in globals() or pages_df is None:\n",
        "        return rebuild_index(force=True)\n",
        "    return \"Index ready (cache loaded).\"\n",
        "\n",
        "def sync_pdfs():\n",
        "    return rebuild_index(force=False)\n",
        "\n",
        "# ----------------------------\n",
        "# Your existing metadata routes\n",
        "# ----------------------------\n",
        "def chapter_answer(doc_filter=\"All\"):\n",
        "    if chapter_df is None or len(chapter_df) == 0:\n",
        "        return \"No chapter-like headings found.\", \"\", \"\", \"\", \"Chapter route\"\n",
        "    df = chapter_df if doc_filter == \"All\" else chapter_df[chapter_df.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No chapter headings in {doc_filter}.\", \"\", \"\", \"\", \"Chapter route\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        cite = f\"[{row.file_name} p.{row.page_number}]\"\n",
        "        lines.append(f\"{cite} {row.heading}\")\n",
        "    answer_text = \"Chapter-like headings I found:\\n\" + \"\\n\".join(lines)\n",
        "    transparency = \"\\n\".join(lines)\n",
        "    return answer_text, \"Metadata\", transparency, answer_text, \"Chapter route\"\n",
        "\n",
        "def pagecount_answer(doc_filter=\"All\"):\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\", \"\", \"Page-count route\"\n",
        "    df = doc_stats if doc_filter == \"All\" else doc_stats[doc_stats.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No stats for {doc_filter}.\", \"\", \"\", \"\", \"Page-count route\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        lines.append(f\"{row.file_name}: {row.total_pages} pages (low-text {row.low_text_pages})\")\n",
        "    ans = \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans, ans, \"Page-count route\"\n",
        "\n",
        "def listdocs_answer():\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\", \"\", \"List docs route\"\n",
        "    lines = [f\"{row.file_name} ({row.total_pages} pages)\" for row in doc_stats.itertuples()]\n",
        "    ans = \"Available PDFs:\\n\" + \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans, ans, \"List docs route\"\n",
        "\n",
        "def author_answer(doc_filter=\"All\"):\n",
        "    if doc_stats is None or len(doc_stats) == 0:\n",
        "        return \"No PDFs indexed yet.\", \"\", \"\", \"\", \"Author route\"\n",
        "    df = doc_stats if doc_filter == \"All\" else doc_stats[doc_stats.file_name == doc_filter]\n",
        "    if len(df) == 0:\n",
        "        return f\"No author info for {doc_filter}.\", \"\", \"\", \"\", \"Author route\"\n",
        "    lines = []\n",
        "    for row in df.itertuples():\n",
        "        lines.append(f\"{row.file_name}: {row.author} (Title: {row.title})\")\n",
        "    ans = \"\\n\".join(lines)\n",
        "    return ans, \"Metadata\", ans, ans, \"Author route\"\n",
        "\n",
        "# ----------------------------\n",
        "# Retrieval + route (unchanged logic)\n",
        "# Returns 5-tuple: retrieval_output, conf, transparency, raw_context_for_llm, retrieval_route\n",
        "# ----------------------------\n",
        "def pipeline_retrieve_and_route(question, top_k=3, doc_filter=\"All\"):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    _ = ensure_index_ready()\n",
        "\n",
        "    if is_list_docs_query(question):\n",
        "        return listdocs_answer()\n",
        "    if is_pagecount_query(question):\n",
        "        return pagecount_answer(doc_filter)\n",
        "    if is_author_query(question):\n",
        "        return author_answer(doc_filter)\n",
        "    if is_chapter_query(question):\n",
        "        return chapter_answer(doc_filter)\n",
        "\n",
        "    res = retrieve_chunks(question, k=int(top_k), doc_filter=doc_filter)\n",
        "    if res[\"error\"]:\n",
        "        return res[\"error\"], \"\", \"\", \"\", \"\"\n",
        "\n",
        "    items = res[\"items\"]\n",
        "    top = items[0]\n",
        "    conf = f\"{confidence_label(top['score'])} (top score {top['score']:.3f})\"\n",
        "\n",
        "    raw_context_for_llm = \"\\n\\n\".join([item[\"chunk_text\"] for item in items])\n",
        "    retrieval_output = \"I found the following information in the documents:\\n\\n\" + raw_context_for_llm\n",
        "    retrieval_route = \"Direct embedding retrieval (no LLM).\"\n",
        "\n",
        "    lines = []\n",
        "    for it in items:\n",
        "        cite = f\"[{it['file_name']} p.{it['page_number']}]\"\n",
        "        lines.append(f\"Score: {it['score']:.3f} | {it['confidence']} | {cite}\\n{it['preview']}\")\n",
        "    transparency = \"\\n\\n\".join(lines)\n",
        "\n",
        "    return retrieval_output, conf, transparency, raw_context_for_llm, retrieval_route\n",
        "\n",
        "# ----------------------------\n",
        "# Public pipeline — matches your earlier usage:\n",
        "# answer, confidence, transparency, grounding = pipeline(...)\n",
        "# ----------------------------\n",
        "def pipeline(question, top_k=3, doc_filter=\"All\", use_gemini=True, stream=False,\n",
        "             model=\"gemini-2.5-flash\"):\n",
        "    \"\"\"\n",
        "    Returns: (answer, confidence, transparency, grounding)\n",
        "    - If metadata route: returns metadata answer (no Gemini).\n",
        "    - If embedding route:\n",
        "        - use_gemini=False => returns retrieval_output\n",
        "        - use_gemini=True  => returns Gemini synthesized answer based ONLY on retrieved context\n",
        "    \"\"\"\n",
        "    retrieval_output, conf, transparency, raw_ctx, route = pipeline_retrieve_and_route(\n",
        "        question, top_k=top_k, doc_filter=doc_filter\n",
        "    )\n",
        "\n",
        "    # Metadata routes should not call Gemini (already a complete deterministic answer)\n",
        "    if route in (\"Chapter route\", \"Page-count route\", \"List docs route\", \"Author route\"):\n",
        "        return retrieval_output, conf, transparency, route\n",
        "\n",
        "    # Errors\n",
        "    if not conf and not transparency and not raw_ctx and route == \"\":\n",
        "        return retrieval_output, conf, transparency, route\n",
        "\n",
        "    if not use_gemini:\n",
        "        return retrieval_output, conf, transparency, route\n",
        "\n",
        "    # Gemini synthesis (context-only)\n",
        "    try:\n",
        "        llm_answer = rag_with_gemini_from_context(\n",
        "            question=question,\n",
        "            raw_context=raw_ctx,\n",
        "            transparency=transparency,\n",
        "            model=model,\n",
        "            max_output_tokens=512,\n",
        "            temperature=0.2,\n",
        "            stream=stream,\n",
        "        )\n",
        "        grounding = \"RAG with Gemini (context-only).\"\n",
        "        return llm_answer, conf, transparency, grounding\n",
        "    except Exception as e:\n",
        "        # Safe fallback: at least return retrieval output\n",
        "        fallback = f\"[Gemini error: {e}]\\n\\n{retrieval_output}\"\n",
        "        return fallback, conf, transparency, \"Fallback to retrieval (Gemini failed).\"\n",
        "\n",
        "print(\"Use: pipeline(question, top_k=3, doc_filter='All', use_gemini=True/False, stream=False)\")\n",
        "print(\"Also available: sync_pdfs() / rebuild_index(force=True)\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use: pipeline(question, top_k=3, doc_filter='All', use_gemini=True/False, stream=False)\n",
            "Also available: sync_pdfs() / rebuild_index(force=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7d90cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `ui_query` function to orchestrate the RAG workflow and then setting up the Gradio UI in Cell 11. This includes calling the `pipeline_retrieve_and_route` function, conditionally using the Gemini model for synthesis, and displaying the raw retrieved context alongside the synthesized answer in the Gradio interface. The `gradio` library is already imported as `gr` in a previous cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "79610a0b",
        "outputId": "83c0d582-1c64-4618-d92d-86290a1319c0"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Cell 11 — Gradio UI (Fixed for google-genai / Fix 2)\n",
        "\n",
        "# ---- Safety: make sure index is loaded so dropdown has choices ----\n",
        "try:\n",
        "    ensure_index_ready()\n",
        "except Exception as e:\n",
        "    print(f\"Index not ready yet: {e}\")\n",
        "\n",
        "METADATA_ROUTES = {\"Chapter route\", \"Page-count route\", \"List docs route\", \"Author route\"}\n",
        "\n",
        "def refresh_doc_choices():\n",
        "    # pdf_paths may be list[Path]\n",
        "    choices = [\"All\"]\n",
        "    if \"pdf_paths\" in globals() and pdf_paths:\n",
        "        choices += [p.name for p in pdf_paths]\n",
        "    return choices\n",
        "\n",
        "def rebuild_ui():\n",
        "    msg = rebuild_index(force=True)\n",
        "    return msg, gr.update(choices=refresh_doc_choices(), value=\"All\")\n",
        "\n",
        "def sync_ui():\n",
        "    msg = sync_pdfs()\n",
        "    return msg, gr.update(choices=refresh_doc_choices(), value=\"All\")\n",
        "\n",
        "def ui_query(question, top_k, doc_filter, use_gemini):\n",
        "    if not str(question).strip():\n",
        "        return \"Please enter a question.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    # Retrieve + route\n",
        "    retrieval_output, confidence, transparency, raw_context_for_llm, retrieval_route = pipeline_retrieve_and_route(\n",
        "        question, top_k, doc_filter\n",
        "    )\n",
        "\n",
        "    # Basic error handling (keep 5 outputs always)\n",
        "    if not retrieval_route and (not confidence and not transparency and not raw_context_for_llm):\n",
        "        # likely retrieval error returned in retrieval_output\n",
        "        return retrieval_output, confidence, transparency, raw_context_for_llm, retrieval_route\n",
        "\n",
        "    if isinstance(retrieval_output, str) and (\n",
        "        retrieval_output.startswith(\"Error:\") or\n",
        "        retrieval_output.startswith(\"No PDFs found\") or\n",
        "        retrieval_output.startswith(\"Index rebuild failed\")\n",
        "    ):\n",
        "        return retrieval_output, \"\", \"\", \"\", retrieval_route\n",
        "\n",
        "    # Default: direct retrieval\n",
        "    answer = retrieval_output\n",
        "    grounding = retrieval_route\n",
        "\n",
        "    # Skip Gemini on metadata routes (already deterministic)\n",
        "    if retrieval_route in METADATA_ROUTES:\n",
        "        grounding = f\"{retrieval_route} (Gemini skipped for metadata).\"\n",
        "        return answer, confidence, transparency, raw_context_for_llm, grounding\n",
        "\n",
        "    # Gemini synthesis (Fix 2) — ONLY if requested and we have context\n",
        "    if use_gemini:\n",
        "        if raw_context_for_llm and str(raw_context_for_llm).strip():\n",
        "            try:\n",
        "                answer = rag_with_gemini_from_context(\n",
        "                    question=question,\n",
        "                    raw_context=raw_context_for_llm,\n",
        "                    transparency=transparency,\n",
        "                    model=\"gemini-2.5-flash\",\n",
        "                    max_output_tokens=512,\n",
        "                    temperature=0.2,\n",
        "                    stream=False,   # set True if you want token streaming in notebook logs\n",
        "                )\n",
        "                grounding = \"RAG with Gemini (context-only).\"\n",
        "            except Exception as e:\n",
        "                answer = f\"[Gemini error: {e}]\\n\\n{retrieval_output}\"\n",
        "                grounding = \"Fallback to retrieval (Gemini failed).\"\n",
        "        else:\n",
        "            grounding = \"No context found (retrieval only).\"\n",
        "\n",
        "    return answer, confidence, transparency, raw_context_for_llm, grounding\n",
        "\n",
        "\n",
        "# ---- Gradio Interface ----\n",
        "with gr.Blocks(title=\"PDF RAG System\") as demo:\n",
        "    gr.Markdown(\"# PDF RAG with Gemini (google-genai) + FAISS\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Configuration\")\n",
        "            rebuild_btn = gr.Button(\"Rebuild Index (clears cache)\")\n",
        "            sync_btn = gr.Button(\"Sync PDFs (uses cache if unchanged)\")\n",
        "            status_output = gr.Textbox(label=\"Index Status\", interactive=False)\n",
        "\n",
        "            doc_filter_dropdown = gr.Dropdown(\n",
        "                label=\"Filter by PDF Document\",\n",
        "                choices=refresh_doc_choices(),\n",
        "                value=\"All\",\n",
        "                interactive=True,\n",
        "                allow_custom_value=False,\n",
        "            )\n",
        "\n",
        "            top_k_slider = gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of Chunks (k)\")\n",
        "            use_gemini_checkbox = gr.Checkbox(label=\"Use Gemini for Synthesis\", value=True)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## Query\")\n",
        "            question_input = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., What is software testing strategies?\")\n",
        "            submit_btn = gr.Button(\"Ask PDF\")\n",
        "\n",
        "            gr.Markdown(\"## Results\")\n",
        "            answer_output = gr.Textbox(label=\"Answer\", interactive=False, lines=10)\n",
        "            confidence_output = gr.Textbox(label=\"Confidence\", interactive=False)\n",
        "            grounding_output = gr.Textbox(label=\"Grounding / Route\", interactive=False)\n",
        "            raw_retrieved_context_display = gr.Textbox(label=\"Raw Retrieved Context\", interactive=False, lines=10)\n",
        "            transparency_output = gr.Textbox(label=\"Retrieval Transparency\", interactive=False, lines=10)\n",
        "\n",
        "    # Event Handlers (also refresh dropdown on rebuild/sync)\n",
        "    rebuild_btn.click(rebuild_ui, outputs=[status_output, doc_filter_dropdown])\n",
        "    sync_btn.click(sync_ui, outputs=[status_output, doc_filter_dropdown])\n",
        "\n",
        "    submit_btn.click(\n",
        "        ui_query,\n",
        "        inputs=[question_input, top_k_slider, doc_filter_dropdown, use_gemini_checkbox],\n",
        "        outputs=[answer_output, confidence_output, transparency_output, raw_retrieved_context_display, grounding_output]\n",
        "    )\n",
        "    question_input.submit(\n",
        "        ui_query,\n",
        "        inputs=[question_input, top_k_slider, doc_filter_dropdown, use_gemini_checkbox],\n",
        "        outputs=[answer_output, confidence_output, transparency_output, raw_retrieved_context_display, grounding_output]\n",
        "    )\n",
        "\n",
        "# Optional: better for longer calls\n",
        "demo.queue(default_concurrency_limit=1)\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://325fc87c8af807b1f0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://325fc87c8af807b1f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 766, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 355, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2157, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1634, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 1038, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4023163973.py\", line 33, in ui_query\n",
            "    retrieval_output, confidence, transparency, raw_context_for_llm, retrieval_route = pipeline_retrieve_and_route(\n",
            "                                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2926357884.py\", line 189, in pipeline_retrieve_and_route\n",
            "    res = retrieve_chunks(question, k=int(top_k), doc_filter=doc_filter)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2409111226.py\", line 33, in retrieve_chunks\n",
            "    q_emb = model.encode([q], convert_to_numpy=True, show_progress_bar=False).astype(\"float32\")\n",
            "            ^^^^^^^^^^^^\n",
            "AttributeError: 'GenerativeModel' object has no attribute 'encode'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://325fc87c8af807b1f0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}